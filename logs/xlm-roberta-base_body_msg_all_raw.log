2022-11-21 09:57:00,547 | INFO: Script args:
{'txt_field': 'body', 'splits': ['train', 'val', 'test'], 'dataset': 'msg_all', 'model_checkpoint': 'xlm-roberta-base', 'num_epochs': 50, 'batch_size': 64, 'resume_from_checkpoint': True}
2022-11-21 09:57:00,548 | INFO: Loading dataset
2022-11-21 09:57:01,090 | WARNING: Using custom data configuration default-08f7f50fe15052cc
2022-11-21 09:57:01,168 | WARNING: Found cached dataset parquet (/cluster/home/erlenoi/.cache/huggingface/datasets/parquet/default-08f7f50fe15052cc/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)
2022-11-21 09:57:03,265 | INFO: Loading tokenizer
2022-11-21 09:57:07,989 | INFO: Tokenizing dataset on body
2022-11-21 09:57:07,992 | WARNING: Loading cached processed dataset at /cluster/home/erlenoi/.cache/huggingface/datasets/parquet/default-08f7f50fe15052cc/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec/cache-ca32701f7892ca5f.arrow
2022-11-21 09:57:08,015 | WARNING: Loading cached processed dataset at /cluster/home/erlenoi/.cache/huggingface/datasets/parquet/default-08f7f50fe15052cc/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec/cache-cce0c587e27184fc.arrow
2022-11-21 09:57:08,064 | WARNING: Loading cached processed dataset at /cluster/home/erlenoi/.cache/huggingface/datasets/parquet/default-08f7f50fe15052cc/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec/cache-d66a68bf2cdfc557.arrow
2022-11-21 09:57:08,107 | WARNING: Loading cached processed dataset at /cluster/home/erlenoi/.cache/huggingface/datasets/parquet/default-08f7f50fe15052cc/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec/cache-c9981169e3d45882.arrow
2022-11-21 09:57:08,671 | WARNING: Loading cached processed dataset at /cluster/home/erlenoi/.cache/huggingface/datasets/parquet/default-08f7f50fe15052cc/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec/cache-b77d3b2f5ca7974e.arrow
2022-11-21 09:57:10,599 | WARNING: Loading cached processed dataset at /cluster/home/erlenoi/.cache/huggingface/datasets/parquet/default-08f7f50fe15052cc/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec/cache-d9f4a351692aae6c.arrow
2022-11-21 09:57:12,303 | INFO: Longest body in train (words): 14424
2022-11-21 09:57:12,303 | INFO: Creating DataPaddingCollator
2022-11-21 09:57:12,303 | INFO: Loading autoconfig
2022-11-21 09:57:12,790 | INFO: Overriden AutoConfig:
XLMRobertaConfig {
  "_name_or_path": "xlm-roberta-base",
  "architectures": [
    "XLMRobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "positive",
    "1": "neutral",
    "2": "negative"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "negative": 2,
    "neutral": 1,
    "positive": 0
  },
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "xlm-roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_past": true,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.18.0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 250002
}

2022-11-21 09:57:12,790 | INFO: Loading model
2022-11-21 09:57:14,751 | INFO: Dividing and OHE labels
2022-11-21 09:57:44,125 | INFO: Post process features
2022-11-21 09:57:46,766 | INFO: {'labels': Value(dtype='int64', id=None), 'input_ids': Sequence(feature=Value(dtype='int32', id=None), length=-1, id=None), 'attention_mask': Sequence(feature=Value(dtype='int8', id=None), length=-1, id=None)}
2022-11-21 09:57:55,687 | INFO: Setup Trainer
2022-11-21 09:57:56,976 | INFO: Loading local model and resuming training
